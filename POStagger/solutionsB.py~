import sys
import nltk
import math

#this function takes the words from the training data and returns a python list of all of the words that occur more than 5 times
#wbrown is a python list where every element is a python list of the words of a particular sentence
def calc_known(wbrown):
    wordcount = {}
    

    #go thru sentence by sentence, word by word, updating wordcount dictionary
    for sentence in wbrown:
        for word in sentence:
            if word in wordcount:
                wordcount[word] += 1
            else:
                wordcount[word] = 1

    print 'wordcounts done'

    knownwords = []
    
    #if a given word has more than 5 occurrances, add it to knownwords
    for key,value in wordcount.items():
        if value > 5:
            knownwords.append(key) 
    

    print 'knownwords done'

    return knownwords

#this function takes a set of sentences and a set of words that should not be marked '_RARE_'
#brown is a python list where every element is a python list of the words of a particular sentence
#and outputs a version of the set of sentences with rare words marked '_RARE_'
def replace_rare(brown, knownwords):
    
    rare = []
   
    knownwordsSet = frozenset(knownwords)
    #knownwordsDict = {knownword: 1 for knownword in knownwords}
   
    for sentence in brown:
        newsentence = []
        for word in sentence:
            if word in knownwordsSet:
                newsentence.append(word)
            else:
                newsentence.append('_RARE_')
#importagged in the WORD/TAG format 
#brown is a list where every element is a list of words
#taglist is from the return of calc_emissions()
#knownwords is from the the return of calc_knownwords()
#qvalues is from the return of calc_trigrams
#evalues is from the return of calc_emissions() 
#tagged is a list of tagged sentences in the format "WORD/TAG". Each sentence is a string, not a list of tokens.
def viterbi(brown, taglist, knownwords, qvalues, evalues):
    brown_smoothed = replace_rare(brown, knownwords)
    
    sentenceTags = []

    for sentence in brown_smoothed:
       

        #dictionary to hold viterbi values of 
        #keys are a 2 tuple (k,(u, v)) where k is word#, (u,v) is tag touple  
        #values are a 2 tuple (probability, backpointer)
        vittable = {}

        #initialize k=0 values for table
        for tag1 in taglist:
            for tag2 in taglist:
                if tag1 == tag2 == '*':
                    vittable[0, (tag1, tag2)] = (0, None)
                else:
                    vittable[0, (tag1, tag2)] = (-1000, None)
                #print (0, (tag1, tag2)), ' : ', vittable[0, (tag1,tag2)]

        n = len(sentence)
        #loop through sentence to fill in table
        for k in range(1, n):
            for u in taglist:
                for v in taglist:
                    
                    #initialize 'pi' probability to very low number, so any decent tag for w is better 
                    pi = -100000
                    bp = None
                    for w in taglist:
                        #print 'vittable: ', vittable[k-1, (w,u)][0], 'qvalue: ', qvalues.get((v, w, u), -1000), 'evalues: ', evalues.get((sentence[k], v), -1000)
                        piTemp = vittable[k-1, (w, u)][0] + qvalues.get((v, w, u), -1000) + evalues.get((sentence[k],v), -1000)
                        if piTemp > pi:
                            pi = piTemp
                            bp = w
                    vittable[k, (u, v)] = (pi, bp)
                    #print 'k,u,v:', (k, (u, v)), 'and values: ', (pi, bp)
         
        thisSentenceTags = []
         
        tagged = []
        
        #find best trigram ending in STOP, follow backpointers
        finalprob = -100000
        winner = None
        for u in taglist:
            for v in taglist:
                #print 'finalloop:', (n-1, (u, v))
                finalprobTemp = vittable[n-1, (u, v)][0] + qvalues.get(('STOP', u, v), -1000)
                if finalprobTemp > finalprob:
                    #print 'herE'
                    finalprob = finalprobTemp
                    winner = (n-1, (u, v))
        
        #put v, u into the sentence tags
        thisSentenceTags.append(winner[1][1])
       
        for k in range(n-2, -1, -1):
            thisSentenceTags.append(winner[1][0])
            winner = (k, (vittable[winner][1], winner[1][0]))
            
        thisSentenceTags.reverse()
        print thisSentenceTags
        
        taggedSentence = ""
        #for x in range(0, n):
                #taggedSentence += sentence[x] + '/' + thisSentenceTags[x] + ' '

        tagged.append(taggedSentence)





    return tagged

#this function takes the output of viterbi() and outputs it
def q5_output(tagged):
    outfile = open('B5.txt', 'w')
    for sentence in tagged:
        outfile.write(sentence)
    outfile.close()

#this function uses nltk to create the taggers described in question 6
#brown is the data to be tagged
#tagged is a list of tagged sentences. Each sentence is in the WORD/TAG format and is a string rather than a list of tokens.
def nltk_tagger(brown):
    tagged = []
    return tagged

def q6_output(tagged):
    outfile = open('B4.txt', 'w')

    for sentence in tagged:
        outfile.write(sentence)
    outfile.close()

#a function that returns two lists, one of the brown data (words only) and another of the brown data (tags only) 
def split_wordtags(brown_train):
                   
    wbrown = []
    tbrown = []
   
    for sentence in brown_train:
        words = sentence.split()
        words.insert(0, '*/*')
        words.insert(0, '*/*')
        words.append('STOP/STOP')
        wordList = []
        tagList = []
        for word in words:
            splitword = word.rsplit("/", 1)
            wordList.append(splitword[0])
            tagList.append(splitword[1])
        
        wbrown.append(wordList)
        tbrown.append(tagList)

    return wbrown, tbrown

def main():
    #open Brown training data
    infile = open("Brown_tagged_train.txt", "r")
    brown_train = infile.readlines()
    infile.close()

    #split words and tags, and add start and stop symbols (question 1)
    wbrown, tbrown = split_wordtags(brown_train)
           
    #calculate trigram probabilities (question 2)
    qvalues = calc_trigrams(tbrown)
    
    #question 2 output
    q2_output(qvalues)

    #calculate list of words with count > 5 (question 3)
    knownwords = calc_known(wbrown)

    #get a version of wbrown with rare words replace with '_RARE_' (question 3)
    wbrown_rare = replace_rare(wbrown, knownwords)

    #question 3 output
    q3_output(wbrown_rare)

    #calculate emission probabilities (question 4)
    evalues, taglist = calc_emission(wbrown_rare, tbrown)

    #question 4 output
    q4_output(evalues)

    #delete unneceessary data
    del brown_train
    del wbrown
    del tbrown
    del wbrown_rare

    #open Brown development data (question 5)
    infile = open("Brown_dev.txt", "r")
    brown_dev = infile.readlines()
    infile.close()
    
    for x in range (0, len(brown_dev)):
        brown_dev[x] = nltk.word_tokenize(brown_dev[x])


    #replace rare words in brown_dev (question 5)
    brown_dev_rare = replace_rare(brown_dev, knownwords)

    #do viterbi on brown_dev (question 5)
    viterbi_tagged = viterbi(brown_dev, taglist, knownwords, qvalues, evalues)

    #question 5 output
    q5_output(viterbi_tagged)

    #do nltk tagging here
    nltk_tagged = nltk_tagger(brown_dev)
    
    #question 6 output
    q6_output(nltk_tagged)
if __name__ == "__main__": main()
